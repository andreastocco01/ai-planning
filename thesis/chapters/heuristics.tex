\chapter{Proposed Approaches}
\label{ch:heuristics}
Heuristics are algorithms to determine a near-optimal solution to a problem. In certain scenarios,
exact methods aren't able to provide the optimal solution in a reasonable amount of time or they can't find it
at all. In contrast, a heuristic is generally capable of offering a solution that is more or less close to
the optimal one.
While there is no assurance regarding the optimality of the provided solution, it may still be considered
acceptable in some instances.
This chapter focuses on explaining the algorithms developed, while the following one presents their evaluation.\\
Before presenting the algorithms, it is useful to introduce a fundamental function used by all of them.
An action is applicable only if all its preconditions are satisfied in the current state.
When applied, it produces a new state by adding its effects to the current one.
The pseudocode for this function is shown in Algorithm \ref{alg:apply}.

\begin{algorithm}
	\caption{Apply}
	\label{alg:apply}
	\hspace*{0.5em} \textbf{Output}: $newState$
	\begin{algorithmic}[1]
		\Procedure{apply}{$currentState, actionToApply$}
		\State $newState \gets \Call{copy}{currentState}$
		\ForAll{$fact \in actionToApply.effects$}
		\If{$fact \notin newState$}
		\State $newState \gets newState \cup \{fact\}$
		\EndIf
		\EndFor
		\State \textbf{return} $newState$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

To illustrate the behavior of the heuristics, a simple planning instance is introduced
as a running example throughout this chapter.

\begin{definition}[Running Example]
	The instance consists of an initial state $s_i$, a goal state $s_g$, a set of intermediate states $s_1, \dots, s_{13}$,
	and a set of actions $a_1, \dots, a_{15}$, each with uniform cost 1.
	Each state can take only two values, true or false, and each action has exactly one precondition and one effect.
	An action can be applied only if its precondition state is true; once applied, its effect state becomes true as well.
\end{definition}

The structure of the instance is shown in Figure~\ref{fig:running_example}.
It is represented as a graph, where each circle denotes a state and each edge corresponds to an action.
Although the edges are drawn as undirected, each action conceptually represents a transition from its precondition to its effect.
For example, action $a_2$ is applicable in $s_i$, and its effect is to reach $s_2$.
Subsequent figures reuse this example to highlight the differences among heuristics,
with each edge labeled by the corresponding heuristic cost.

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.75\linewidth}
	\input{images/graph.pdf_tex}
	\caption{Running Example}
	\label{fig:running_example}
\end{figure}

\section{Random}
This is the simplest algorithm implemented, and it is used as a baseline for the evaluation.
At each iteration, it applies a random action from the applicable ones.
It is not a sophisticated or informed solution but this strategy can be useful
in some cases because, by chance, a good plan may be found.
Algorithm \ref{alg:random} shows the pseudocode for it.

\begin{algorithm}
	\caption{Random}
	\label{alg:random}
	\hspace*{0.5em} \textbf{Output}: $solution$
	\begin{algorithmic}[1]
		\Procedure{random}{$planningTask$}
		\State $currentState \gets planningTask.initialState$
		\State $solution \gets [\ ]$ \Comment{Initialize empty solution}
		\While {$planningTask.goalState \not\subseteq currentState$}
		\State $possibleActions \gets \{a \in A \mid pre(a) \subseteq currentState\}$
		\If {$\Call{empty}{possibleActions}$}
		\State \textbf{return} $failure$ \Comment{Infeasible problem}
		\EndIf
		\State $actionToApply \gets \Call{randomChoice}{possibleActions}$
		\State $currentState \gets \Call{apply}{currentState, actionToApply}$
		\State \Call{append}{$solution$, $actionToApply$}
		\EndWhile
		\State \textbf{return} $solution$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

It is important to notice that if \textit{possibleActions} is empty, then
a solution to the planning task cannot exist. As said in Chapter \ref{ch:intro},
the problems are delete-free|once a fact becomes true in the current state, it remains
true until the end.
Therefore, if there are no actions to apply, it does not mean that the algorithm has reached
a dead end that could be avoided by applying different actions in a different order.
Rather, it indicates that no sequence of actions can lead to the goal state, and therefore no plan exists.
The natural evolution of this approach is to avoid choosing randomly the next action to apply and, instead, use
a strategy to select the action that is more likely to lead to a better plan.

\section{Greedy}
A greedy algorithm is a heuristic that attempts to find an optimal solution by selecting the locally
best possible choice at each iteration. For instance, in our case, the next action to apply will
always be the one with the minimum cost. In the case of a tie, a random action among those with the same cost
will be chosen.
Algorithm \ref{alg:greedy} shows the pseudocode for this algorithm. The omitted parts are identical to those in Algorithm \ref{alg:random}.

\begin{algorithm}
	\caption{Greedy}
	\label{alg:greedy}
	\hspace*{0.5em} \textbf{Output}: $solution$
	\begin{algorithmic}[1]
		\Procedure{greedy}{$planningTask$}
		\State \dots
		\State $possibleActions \gets \{a \in A \mid pre(a) \subseteq currentState\}$
		\If {$\Call{empty}{possibleActions}$}
		\State \textbf{return} $failure$
		\EndIf
		\State $actionToApply \gets \Call{minimumCostAction}{possibleActions}$
		\State \dots
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Instead of randomly choosing the next action to apply, here the action with minimum cost is always selected.
For this and all subsequent algorithms, if multiple eligible actions share the same minimum (heuristic) cost,
one of them is selected at random.

\section{Revised $h^{\max}$ Heuristic}
\label{sec:hmax}
The $h^{\max}$ heuristic is one of the most well-known delete-relaxation heuristics in the literature.
It is an \textit{admissible} heuristic, meaning that it never overestimates the cost of reaching the goal.
A formal definition is given in Equations \ref{eq:hmax} and \ref{eq:cost}.

\begin{equation}
	\label{eq:hmax}
	h^{\max}\left(p;s\right) \coloneqq \begin{cases}
		0                                                                & \text{if $p \in s$} \\
		\text{min}_{a \in O\left(p\right)}\left[h\left(a;s\right)\right] & \text{otherwise}
	\end{cases}
\end{equation}

where $h^{\max}\left(p;s\right)$ stands for an estimate of the cost of achieving the fact \textit{p} from
the current state \textit{s}, $O\left(p\right)$ is the set of actions $\{a \in A \mid p \in \textit{eff(a)}\}$, and

\begin{equation}
	\label{eq:cost}
	h\left(a;s\right) \coloneqq cost(a) + \text{max}_{q \in pre\left(a\right)} \left[h^{\max}(q, s)\right]
\end{equation}

stands for the cost of achieving the preconditions of an action \textit{a} and applying it.\\
The idea behind the use of this heuristic is similar to that of the Greedy approach.
While in the Greedy approach the action with the minimum actual cost is chosen, in this case,
the action with the minimum heuristic cost is selected at each iteration.
This thesis presents an implementation of the $h^{\max}$ heuristic that goes a step further.
By definition, any action applicable in the current state has a heuristic cost equal to
its own cost, since all its preconditions are satisfied|regardless of whether the action is
actually useful for reaching the goal state or not.
The implemented version performs a technique called \textit{pruning}, which consists of assigning
an infinite cost to useless actions|indicating that these actions are not helpful in finding a good plan.
As a result, such actions are effectively excluded from consideration when selecting the next action to apply,
while the remaining ones retain their correct heuristic cost.
The pseudocode for this algorithm is presented in Algorithm \ref{alg:hmax} and \ref{alg:greedyhmax}.

\begin{algorithm}
	\caption{$h^{\max}$ heuristic}
	\label{alg:hmax}
	\hspace*{0.5em} \textbf{Output}: $minHcost$
	\begin{algorithmic}[1]
		\Procedure{hmax}{$currentState, fact$}
		\If {$fact \in currentState$}
		\State \textbf{return} 0 \Comment{Base case}
		\EndIf
		\State $actions \gets \{a \in A \mid fact \in \textit{eff(a)}\}$ \Comment{Get all the actions having \textit{fact} as effect}
		\If {$\Call{empty}{actions}$}
		\State \textbf{return} $+\infty$ \Comment{The fact is unreachable}
		\EndIf
		\State $minHcost \gets +\infty$
		\ForAll{$action\in actions$}
		\State $maxCost \gets 0$
		\ForAll {$pre \in action.preconditions$}
		\State $maxCost \gets \Call{max}{maxCost, \Call{hmax}{currentState, pre}}$
		\EndFor
		\State $action.hCost \gets action.cost + maxCost$
		\State $minHcost \gets \Call{min}{minHcost, action.hCost}$
		\EndFor
		\State \textbf{return} $minHcost$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Greedy search with $h^{\max}$ heuristic}
	\label{alg:greedyhmax}
	\hspace*{0.5em} \textbf{Output}: $solution$
	\begin{algorithmic}[1]
		\Procedure{greedyHmax}{$planningTask$}
		\State \dots
		\ForAll {$a \in A$}
		\State $a.hCost \gets +\infty$ \Comment{Initialize actions' heuristic cost}
		\EndFor
		\State $estimatedCost \gets 0$
		\ForAll {$goal \in planningTask.goalState$}
		\State $estimatedCost \gets \Call{max}{estimatedCost, \Call{hmax}{currentState, goal}}$
		\EndFor
		\State $possibleActions \gets \{a \in A \mid pre(a) \subseteq currentState\}$
		\If {$\Call{empty}{possibleActions}$}
		\State \textbf{return} $failure$
		\EndIf
		\State $actionToApply \gets \Call{minimumHcostAction}{possibleActions}$
		\State \dots
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Starting from each fact in the goal state, \textsc{hmax} is invoked to perform backward cost propagation.
Through this process, actions that do not contribute to achieving the goal will implicitly retain their initial cost of $+\infty$,
effectively marking them as useless. When selecting the next action to apply, any action with infinite cost is ignored,
and the one with the lowest heuristic cost among the remaining options is chosen.\\
Figure \ref{fig:hmax_scheme} illustrates the behaviour of the $h^{\max}$ heuristic without the pruning technique.
When the heuristic is applied from the initial state, the actions $a_1$, $a_2$, and $a_3$ are all deemed applicable and assigned heuristic costs
equal to their actual cost. However, since multiple actions have the same cost, the tie-breaking mechanism (e.g., random choice) may lead the planner
to select a suboptimal branch|such as choosing $a_1$ or $a_3$| which does not contribute toward reaching the goal efficiently.

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.75\linewidth}
	\input{images/hmax.pdf_tex}
	\caption{Running example evaluated with the $h^{\max}$ heuristic (without pruning)}
	\label{fig:hmax_scheme}
\end{figure}

Figure \ref{fig:hmax_pruning_scheme} shows the same planning instance, but this time using the $h^{\max}$ heuristic augmented with the pruning technique.
Here, actions that do not contribute to achieving the goal are implicitly assigned a heuristic cost of $+\infty$. As a result, the planner deterministically
selects $a_2$ from the initial state, as it is the only action with a finite cost.
The heuristic is recomputed after each action, updating costs based on the new state. This guided progression ensures that only goal-relevant actions are
considered. For this instance, a possible suboptimal plan that the pruning-enhanced heuristic might produce is $\langle a_2, a_6, a_{10}, a_{13}, a_{15}\rangle$.
This occurs because, from state $s_6$, both $a_{10}$ and $a_{11}$ have the same heuristic cost of one, so the decision between them is made randomly.

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.75\linewidth}
	\input{images/hmax_pruning.pdf_tex}
	\caption{Running example evaluated with the $h^{\max}$ heuristic with pruning}
	\label{fig:hmax_pruning_scheme}
\end{figure}

As previously mentioned, any action that can be applied from
the current state will always have a heuristic cost equal to its actual cost.
Therefore, this algorithm can be seen as a Greedy + Pruning approach: it selects the next action based on the action's cost,
while discarding useless actions.

\section{$h^{\max}$ Heuristic + Lookahead}
As mentioned in the previous section, the presented algorithm can be considered a Greedy + Pruning approach,
since the full potential of the heuristic cannot be exploited|the heuristic cost of the applicable actions is always equal
to the actions' actual cost.
To evaluate its performance, a new version of the heuristic is proposed.
Instead of selecting the action with the minimum cost, the algorithm tries each possible action,
recomputes the \textsc{hmax} value for each resulting state, and finally applies the action that leads to the state with the lowest
new heuristic value.
The pseudocode for this approach is shown in Algorithm \ref{alg:lookahead} and \ref{alg:hmaxlookahead}.

\begin{algorithm}
	\caption{Lookahead}
	\label{alg:lookahead}
	\hspace*{0.5em} \textbf{Output}: $actionToApply$
	\begin{algorithmic}[1]
		\Procedure{lookahead}{$currentState, possibleActions$}
		\ForAll{$action\in possibleActions$}
		\State $newState \gets \Call{apply}{currentState, action}$
		\State $estimatedCost \gets 0$
		\ForAll {$goal \in planningTask.goalState$}
		\State $estimatedCost \gets \Call{max}{estimatedCost, \Call{hmax}{newState, goal}}$
		\EndFor
		\State $action.hCost \gets action.hCost + estimatedCost$
		\EndFor
		\State $actionToApply \gets \Call{minimumHcostAction}{possibleActions}$
		\State \textbf{return} $actionToApply$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{$h^{\max}$ heuristic with lookahead}
	\label{alg:hmaxlookahead}
	\hspace*{0.5em} \textbf{Output}: $solution$
	\begin{algorithmic}[1]
		\Procedure{hmaxLookahead}{$planningTask$}
		\State \dots
		\State $estimatedCost \gets 0$
		\ForAll {$goal \in planningTask.goalState$}
		\State $estimatedCost \gets \Call{max}{estimatedCost, \Call{hmax}{currentState, goal}}$
		\EndFor
		\State $possibleActions \gets \{a \in A \mid pre(a) \subseteq currentState\}$
		\If {$\Call{empty}{possibleActions}$}
		\State \textbf{return} $failure$
		\EndIf
		\State $actionToApply \gets \Call{lookahead}{currentState, possibleActions}$
		\State \dots
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

The algorithm "looks ahead" at the heuristic cost of the state resulting from each possible action,
and then applies only the action that appears to be the most convenient.

\section{Shortest Path Heuristic}
\label{sec:shortestpath}
In this section, a completely new approach is introduced. As discussed in Section \ref{sec:hmax},
the modified version of the $h^{\max}$ heuristic ultimately behaves like a Greedy + Pruning algorithm, limiting its effectiveness.
Here, we aim to use the same backward propagation mechanism, but with a different heuristic cost assigned to each action.
The core idea is to assign each action a cost that estimates its actual distance to the goal.
Specifically, the cost assigned to an action corresponds to the cost of the \textit{shortest path} to reach a fact in the goal state.
The mathematical formalism is presented in Equations \ref{eq:sp} and \ref{eq:spcost}:

\begin{equation}
	\label{eq:sp}
	h^{sp}\left(p;s_*\right) \coloneqq \begin{cases}
		0                                                                  & \text{if $p \in s_*$} \\
		\text{min}_{a \in O\left(p\right)}\left[h\left(a;s_*\right)\right] & \text{otherwise}
	\end{cases}
\end{equation}

Here, $h^{sp}\left(p;s_*\right)$ denotes the estimated cost of achieving the fact \textit{p} from the goal state $s_*$.
The set $O\left(p\right)$ represents the actions that produce \textit{p} as an effect,
that is, $O\left(p\right) = \{a \in A \mid p \in \textit{eff(a)}\}$. The function $h\left(a;s_*\right)$ is
defined as follows:

\begin{equation}
	\label{eq:spcost}
	h\left(a;s_*\right) \coloneqq cost(a) + \text{min}_{q \in \textit{eff}\left(a\right)} \left[h^{sp}\left(q;s_*\right)\right]
\end{equation}

This expression estimates the cost of achieving a fact in the goal state $s_*$ by applying
action \textit{a}. The heuristic cost of an action is defined as its own cost plus the minimum estimated cost among its effects.
In this way, each action is assigned a heuristic cost corresponding to the cost of the shortest path to reach
a fact in the goal state.
Figure \ref{fig:sp_scheme} illustrates the same example introduced at the beginning of this chapter, but evaluated with the newly presented heuristic.

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.75\linewidth}
	\input{images/shortest_path.pdf_tex}
	\caption{Running example evaluated with the Shortest Path heuristic}
	\label{fig:sp_scheme}
\end{figure}

In this case, the heuristic cost of each action corresponds to the length of the shortest path required to reach a fact in the goal state.
For example, if the current state is $s_6$, reached by sequentially applying actions $a_2$ and $a_6$, then the next selected action will be $a_{11}$,
as it lies on the shortest path to the goal.
In this case, the resulting plan is always the optimal one: $\langle a_2, a_6, a_{11}, a_{14}\rangle$.
As with the previous method, a pruning strategy is applied: any action that does not contribute to reaching the goal state $s_g$ is
assigned a cost of $+\infty$ and is thus excluded from consideration.
The actual implementation of this algorithm uses a \textit{priority queue} to ensure an
efficient and iterative computation. The pseudocode is presented in Algorithm \ref{alg:sp}.

\begin{algorithm}
	\caption{Shortest Path}
	\label{alg:sp}
	\hspace*{0.5em} \textbf{Output}:
	\begin{algorithmic}[1]
		\Procedure{shortestPath}{$currentState, goalState$}
		\State $factCosts \gets \{+\infty, \dots, +\infty\}$ \Comment{Initialize facts' cost to $+\infty$}
		\State $pq\gets \{\}$ \Comment{Empty priority queue}
		\ForAll {$goal \in goalState$}
		\State $factCosts[goal] \gets 0$
		\State $\Call{push}{pq, goal, 0}$
		\EndFor
		\While {$!\Call{empty}{pq}$}
		\State $fact \gets \Call{pop}{pq}$ \Comment{Get the fact having minimum cost}
		\If {$fact \in currentState$}
		\State \textbf{continue} \Comment{Do not propagate the cost}
		\EndIf
		\State $actions \gets \{a \in A \mid fact \in \textit{eff}\left(a\right)\}$
		\ForAll {$action \in actions$}
		\State $newCost \gets action.cost + factCosts[fact]$
		\If {$newCost >= action.hCost$}
		\State \textbf{continue}
		\EndIf
		\State $action.hCost = newCost$
		\ForAll {$pre \in action.preconditions$}
		\If {$newCost < factCosts[pre]$}
		\State $factCosts[pre] \gets newCost$
		\If {$\Call{has}{pq, pre}$}
		\State $\Call{change}{pq, pre, newCost}$
		\Else
		\State $\Call{push}{pq, pre, newCost}$
		\EndIf
		\EndIf
		\EndFor
		\EndFor
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

The priority queue internally maintains the facts ordered by increasing cost.
Each time a fact is removed from the queue, it is the one with the minimum heuristic cost.
The heuristic cost of each action having the removed fact as an effect is then updated accordingly,
and all of its preconditions are subsequently inserted into the queue.
In this way, all useful actions will be assigned a cost equal to the shortest path to reach a fact in
the goal state, while useless actions will retain an infinite cost.
The usage of this algorithm is analogous to what was presented in Algorithm \ref{alg:greedyhmax}.
